<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Context-DPO: Aligning Language Models for Context-Faithfulness">
  <meta property="og:title" content="Context-DPO: Aligning Language Models for Context-Faithfulness"/>
  <meta property="og:description" content="This project introduces Context-DPO, an alignment method to enhance the context-faithfulness of large language models (LLMs)."/>
  <meta property="og:url" content="https://github.com/byronBBL/Context-DPO"/>
  <meta property="og:image" content="static/images/context-dpo-banner.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="Context-DPO: Aligning LLMs for Context-Faithfulness">
  <meta name="twitter:description" content="Introducing Context-DPO, a new method to improve context adherence in large language models.">
  <meta name="twitter:image" content="static/images/context-dpo-twitter.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="LLM, Context-Faithfulness, Alignment, Context-DPO, ConFiQA, Knowledge Conflicts, NLP">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Context-DPO: Aligning LLMs for Context-Faithfulness</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              Context-DPO: Aligning Language Models for Context-Faithfulness
            </h1>
            <div class="is-size-8">
              <span class="author-block"> <a href="https://byronbbl.github.io/" target="_blank">Baolong Bi</a><sup>1</sup>, </span>
              <span class="author-block"> <a href="https://www.microsoft.com/en-us/research/people/shaohanh/" target="_blank">Shaohan Huang<sup>2</sup>, </span>
              <span class="author-block"> <a href="https://wangywust.github.io/" target="_blank">Yiwei Wang</a><sup>3</sup>, </span>
              <span class="author-block">Tianchi Yang<sup>2</sup>, Zihan Zhang<sup>2</sup>, Haizhen Huang<sup>2</sup>, Lingrui </span>  <span class="author-block">  Mei<sup>1</sup>, Junfeng Fang<sup>4</sup>, Zehao Li<sup>1</sup>, Furu Wei<sup>2</sup>, Weiwei Deng<sup>2</sup>,
Feng Sun<sup>2</sup>, Qi Zhang<sup>2</sup>,  </span>
              <span class="author-block"> <a href="https://shenghua-liu.github.io/" target="_blank">Shenghua Liu</a><sup>1*</sup>
              </span>      
            </div>
            <div class="is-size-10 publication-authors">
              <span class="author-block"><sup>1</sup>University of Chinese Academy of Sciences</span>
              <span class="author-block"><sup>2</sup>Microsoft Corporation</span><br>
              <span class="author-block"><sup>3</sup>University of California, Merced</span>
              <span class="author-block"><sup>4</sup>National University of Singapore</span>
            </div>
            <br>
            <div class="column has-text-centered">
              
                  <div class="publication-links">
                     <!-- ArXiv abstract Link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2412.15280" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                    </span>
                    
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2412.15280" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/byronBBL/Context-DPO" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                     <!-- Supplementary Models -->
                  <span class="link-block">
                    <a href="https://huggingface.co/collections/Bibaolong/context-faithful-llms-676b783a6f93172ba99751cf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Models</span>
                  </a>
                </span>

                      <!-- Supplementary Datasets -->
                  <span class="link-block">
                    <a href="https://github.com/byronBBL/Context-DPO/tree/master/ConFiQA" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Datasets</span>
                  </a>
                </span>
               
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Generating reliable and accurate responses from large language models (LLMs) hinges on their ability to faithfully adhere to user instructions and integrate retrieved information. 
              Although alignment techniques have proven effective in aligning LLMs with human intentions and values, the dimension of enhancing <strong>context-faithfulness</strong> remains largely underexplored.
            </p>
            <p>
              To bridge this gap, we introduce <strong>Context-DPO</strong>, the first alignment method explicitly designed to reinforce LLMs' faithfulness to contextual information. 
              As part of this effort, we present <strong>ConFiQA</strong>, a novel benchmark crafted to simulate Retrieval-Augmented Generation (RAG) scenarios, replicating real-world knowledge conflicts to rigorously assess context-faithfulness.
            </p>
            <p>
              By utilizing both faithful and stubborn responses to context-driven queries in ConFiQA, Context-DPO aligns LLMs through <strong>DPO</strong>, ensuring they prioritize the provided context during generation.
            </p>
            <p>
              Extensive experimentation validates the effectiveness of Context-DPO, yielding remarkable improvements of <strong>35% to 280%</strong> across popular open-source models. 
              Further analysis confirms that Context-DPO not only enhances context-faithfulness but also preserves the generative strengths of LLMs, offering valuable interpretability into how models leverage contextual knowledge.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


<section class="section" style="background-color: #f9f9f7;">
  <div class="container is-max-desktop">
    <div class="columns is-vcentered" style="display: flex; justify-content: center; max-width: 800px; margin: 0 auto; align-items: center;">
      
      <!-- å·¦ä¾§å›¾ç‰‡ -->
      <div style="flex: 0 0 40%; padding-right: 20px; padding-left: 30px;">  <!-- å›¾ç‰‡åŒºåŸŸå 40% -->
        <img src="static/images/showcase.png" alt="Context Faithfulness" style="width: 100%; height: auto; border: 1px solid #ddd; border-radius: 8px;">
      </div>

      <!-- å³ä¾§æ–‡æœ¬ -->
      <div style="flex: 0 0 60%; padding-left: 20px; padding-left: 30px;">  <!-- æ–‡æœ¬åŒºåŸŸå 60% -->
        <div class="content">
          <h2 class="title is-3">Context-Faithfulness in LLMs</h2>
          <p>
            Generating reliable and accurate responses from large language models (LLMs) hinges on their ability to faithfully adhere to <strong>user instructions</strong> and integrate <strong>retrieved information</strong>. 
            Although alignment techniques have proven effective in aligning LLMs with human intentions and values, the dimension of enhancing <strong>context-faithfulness</strong> remains largely underexplored.
          </p>
        </div>
      </div>
      
    </div>
  </div>
</section>

<section class="section" style="background-color: #eef4fa; padding: 2.5rem 2rem;">
  <div class="container" style="max-width: 900px; margin: 0 auto;">
    <div class="content">
      <h2 class="title is-3" style="color: #2c5282;"><span class="big-emoji">&#x1F4DA;</span> <a href="https://github.com/byronBBL/Context-DPO/tree/master?tab=readme-ov-file#confiqa-format" target="_blank">ConFiQA: A New Benchmark of Context-Faithfulness</a></h2>
      <p style="font-size: 1.1rem; line-height: 1.6;">
        We introduce the <strong>ConFiQA</strong> benchmark to evaluate the <strong>context-faithfulness</strong> of LLMs in real-world <strong>Retrieval-Augmented Generation (RAG)</strong>strong> scenarios involving <strong>knowledge conflicts</strong>. 
        ConFiQA challenges LLMs to navigate conflicting knowledge and prioritize context accuracy, driving advancements in RAG-based AI systems.
        ConFiQA consists of three datasets that reflect varying complexities and reasoning levels:
      </p>
      <ul style="margin-top: 1.5rem; font-size: 1.1rem;">
        <li><strong>QA (Question-Answering):</strong> Single-hop tasks with context containing one counterfactual.</li>
        <li><strong>MR (Multi-hop Reasoning):</strong> Multi-hop tasks involving one counterfactual across multiple reasoning steps.</li>
        <li><strong>MC (Multi-Conflicts):</strong> Multi-hop tasks with context containing multiple counterfactuals, reflecting more complex conflicts.</li>
      </ul>
      <p style="margin-top: 1.5rem; font-style: italic; color: #555;">
      We evaluated popular open-source and also close-source models using ConFiQA and found that context-faithfulness tends to decline as model size increases and training becomes more refined.
      </p>
    </div>
  </div>
</section>

<section class="section" style="padding: 3rem 2rem;">
  <div class="container is-max-desktop">
    
    <!-- æ ‡é¢˜éƒ¨åˆ† -->
    <div class="content" style="max-width: 1000px; margin: 0 auto;">
      <h2 class="title is-3" style="color: #2b2d42; text-align: center;">
        Context-DPO: Aligning LLMs for Context-Faithfulness
      </h2>
      <p style="font-size: 1.1rem; line-height: 1.8; text-align: left;">
        We argue that modern LLMs require alignment specifically to enhance <strong>context-faithfulness</strong>.
        To address this, we propose <strong>Context-DPO</strong>, a novel alignment method that constructs reasoning chains based on single-hop or multi-hop knowledge to generate the <strong>faithful responses</strong> and <strong>stubborn responses</strong>.
        Context-DPO leverages these responses to form preference pairs that guide the model toward context-faithful behavior through Direct Preference Optimization (DPO).
      </p>

      <center><img src="static/images/framework.png" alt="teaser" width="100%"></center>
      <br>
    
    <!-- æ–‡æœ¬éƒ¨åˆ† -->
      <p style="font-size: 1.1rem; line-height: 1.8; text-align: left; margin-top: 0.5rem;">
        ðŸ”¥ Our <strong>Context-DPO</strong> effectively aligns LLMs to improve <strong>context-faithfulness</strong> without compromising their generative capabilities. It consistently outperforms all existing baselines without requiring any external prompt modifications. Specifically, the aligned models achieved substantial improvements compared to their original versions: <strong>35% for <em>Llama2-7B-chat</em></strong>, <strong>78% for <em>Llama3-8B</em></strong>, <strong>151% for <em>Mistral-7B</em></strong>, and <strong>280% for <em>Qwen2-7B</em></strong> ðŸš€.
      </p>
      <br>
      <center><img src="static/images/compare.png" alt="teaser" width="100%"></center>
      <br>
      
      <p style="font-size: 1.1rem; margin-top: 0.5rem; text-align: left;">
        We have open-sourced the aligned
        <a href="https://huggingface.co/collections/Bibaolong/context-faithful-llms-676b783a6f93172ba99751cf" target="_blank" style="color: #007acc; font-weight: bold;">Context-Faithful LLMs</a> by our Context-DPO.
      </p>
    </div>

  

    <!-- è¡¨æ ¼éƒ¨åˆ† -->
    <div class="content" style="max-width: 1000px; margin: 2rem auto;">
      <table style="width: 100%; border-collapse: collapse; border: 1px solid #ddd; font-size: 1.1rem;">
        <thead>
          <tr style="background-color: #e8f4fb; text-align: left;">
            <th style="padding: 10px; border: 1px solid #ddd;">Model Name</th>
            <th style="padding: 10px; border: 1px solid #ddd;">HF Checkpoint</th>
            <th style="padding: 10px; border: 1px solid #ddd;">License</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="padding: 10px; border: 1px solid #ddd;">Context-Faithful-LLaMA-2-7b-chat-hf</td>
            <td style="padding: 10px; border: 1px solid #ddd;">
              ðŸ¤— <a href="https://huggingface.co/Bibaolong/Context-Faithful-LLaMA-2-7b-chat-hf" target="_blank" style="color: #007acc;">Bibaolong/Context-Faithful-LLaMA-2-7b-chat-hf</a>
            </td>
            <td style="padding: 10px; border: 1px solid #ddd;">
              <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank" style="color: #007acc;">Llama2-Chat</a>
            </td>
          </tr>
          <tr>
            <td style="padding: 10px; border: 1px solid #ddd;">Context-Faithful-LLaMA-3-8b-instruct</td>
            <td style="padding: 10px; border: 1px solid #ddd;">
              ðŸ¤— <a href="https://huggingface.co/Bibaolong/Context-Faithful-LLaMA-3-8b-instruct" target="_blank" style="color: #007acc;">Bibaolong/Context-Faithful-LLaMA-3-8b-instruct</a>
            </td>
            <td style="padding: 10px; border: 1px solid #ddd;">
              <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank" style="color: #007acc;">Llama3-Instruct</a>
            </td>
          </tr>
          <tr>
            <td style="padding: 10px; border: 1px solid #ddd;">Context-Faithful-Mistral-7B-instruct</td>
            <td style="padding: 10px; border: 1px solid #ddd;">
              ðŸ¤— <a href="https://huggingface.co/Bibaolong/Context-Faithful-Mistral-7B-instruct-v0.2" target="_blank" style="color: #007acc;">Bibaolong/Context-Faithful-Mistral-7B-instruct-v0.2</a>
            </td>
            <td style="padding: 10px; border: 1px solid #ddd;">
              <a href="https://mistral.ai/contact/" target="_blank" style="color: #007acc;">Mistral-Instruct</a>
            </td>
          </tr>
          <tr>
            <td style="padding: 10px; border: 1px solid #ddd;">Context-Faithful-Qwen2-7B-Instruct</td>
            <td style="padding: 10px; border: 1px solid #ddd;">
              ðŸ¤— <a href="https://huggingface.co/Bibaolong/Context-Faithful-Qwen2-7B-Instruct" target="_blank" style="color: #007acc;">Bibaolong/Context-Faithful-Qwen2-7B-Instruct</a>
            </td>
            <td style="padding: 10px; border: 1px solid #ddd;">
              <a href="https://github.com/QwenLM/Qwen/blob/main/LICENSE" target="_blank" style="color: #007acc;">Qwen-Instruct</a>
            </td>
          </tr>
        </tbody>
      </table>
    </div>
  </div>
</section>

  <section class="section" style="background-color: #f9f5fc; padding: 3rem 2rem;">
  <div class="container is-max-desktop">
    
    <!-- æ ‡é¢˜éƒ¨åˆ† -->
    <div class="content" style="max-width: 1000px; margin: 0 auto;">
      <h2 class="title is-3" style="color: #4a235a; text-align: center;">
        In-depth Exploration of Context-Faithfulness
      </h2>
      <p style="font-size: 1.1rem; line-height: 1.8; text-align: left;">
        Our analysis showcases the transformative impact of <strong>Context-DPO</strong> on enhancing LLMs' <strong>context-faithfulness</strong>. The alignment reduces irrelevant and stubborn responses, leading to a significant rise in context-faithful answers. Through token-level analysis, we observe that aligned models effectively prioritize context-relevant tokens, boosting their probability distributions and improving overall response fidelity.
      </p>
    </div>

    <!-- å›¾è¡¨éƒ¨åˆ† -->
    <figure class="image" style="margin: 2rem auto; text-align: center;">
      <img src="static/images/log_compare.png" alt="Logits Comparison" style="border-radius: 8px; box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1); max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #4a235a; margin-top: 0.5rem;">Figure 1: Average logits of context-faithful tokens, highlighting improvements with Context-DPO.</figcaption>
    </figure>
    <figure class="image" style="margin: 2rem auto; text-align: center;">
      <img src="static/images/logits.png" alt="Kernel Density Estimation" style="border-radius: 8px; box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1); max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #4a235a; margin-top: 0.5rem;">Figure 2: Softmax probability distribution for context-faithful tokens.</figcaption>
    </figure>
    <figure class="image" style="margin: 2rem auto; text-align: center;">
      <img src="static/images/rank2.png" alt="Ranking Distribution" style="border-radius: 8px; box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1); max-width: 100%; height: auto;">
      <figcaption style="font-size: 0.9rem; color: #4a235a; margin-top: 0.5rem;">Figure 3: Frequency of top-ranked context-faithful tokens in aligned models.</figcaption>
    </figure>

    <!-- ç»“å°¾éƒ¨åˆ† -->
    <div class="content" style="max-width: 1000px; margin: 2rem auto;">
      <p style="font-size: 1.1rem; line-height: 1.8; text-align: left;">
        These findings illustrate the internal mechanisms of <strong>Context-DPO</strong>, demonstrating its ability to significantly improve context-faithfulness alignment in LLMs. The results highlight how this alignment boosts the generation frequency of top-ranked context-faithful tokens, enhancing overall response quality without reliance on external methods.
      </p>
    </div>

  </div>
</section>



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{bi2024context,
  title={Context-DPO: Aligning Language Models for Context-Faithfulness},
  author={Bi, Baolong and Huang, Shaohan and Wang, Yiwei and Yang, Tianchi and Zhang, Zihan and Huang, Haizhen and Mei, Lingrui and Fang, Junfeng and Li, Zehao and Wei, Furu and others},
  journal={arXiv preprint arXiv:2412.15280},
  year={2024}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->

</html>
