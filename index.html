<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Context-DPO: Aligning Language Models for Context-Faithfulness">
  <meta name="keywords" content="Context Faithfulness, Direct Preference Optimization, LLM Alignment, Knowledge Conflicts">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Context-DPO: Aligning Language Models for Context-Faithfulness</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/tifa.png">
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              <span class="big-emoji">&#x1F4DA;</span>
              Context-DPO: Aligning Language Models for Context-Faithfulness
            </h1>
            <div class="is-size-5">
              <span class="author-block">
                Baolong Bi<sup>1</sup>, Shaohan Huang<sup>2</sup>, Yiwei Wang<sup>3</sup>, Tianchi Yang<sup>2</sup>, Zihan Zhang<sup>2</sup>, Haizhen Huang<sup>2</sup>, Lingrui Mei<sup>1</sup>, Junfeng Fang<sup>4</sup>, Zehao Li<sup>1</sup>, Furu Wei<sup>2</sup>, Weiwei Deng<sup>2</sup>,
Feng Sun<sup>2</sup>, Qi Zhang<sup>2</sup>, Shenghua Liu<sup>1</sup>           </span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>University of Chinese Academy of Sciences</span><br>
              <span class="author-block"><sup>2</sup>Microsoft Corporation</span><br>
              <span class="author-block"><sup>3</sup>University of California, Merced</span><br>
              <span class="author-block"><sup>4</sup>National University of Singapore</span>
            </div>
            <br>
            <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2412.15280" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="https://huggingface.co/collections/Bibaolong/context-faithful-llms-676b783a6f93172ba99751cf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/byronBBL/Context-DPO" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2412.15280" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Reliable responses from large language models (LLMs) require adherence to user instructions and retrieved information. While alignment techniques enhance LLMs with human intentions, improving <b>context-faithfulness</b> remains under-explored. We propose <b>Context-DPO</b>, the first method designed to enhance LLMs' context-faithfulness through <b>Direct Preference Optimization (DPO)</b>. Our approach leverages <b>ConFiQA</b>, a benchmark simulating Retrieval-Augmented Generation (RAG) scenarios with knowledge conflicts. 
              Context-DPO achieves <b>35% to 280% improvements</b> in faithfulness across popular open-source models. The alignment process preserves LLMs' generative capabilities while significantly enhancing their ability to utilize contextual knowledge.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">BibTeX</h2>
          <pre><code>
@article{bi2024context,
  title={Context-DPO: Aligning Language Models for Context-Faithfulness},
  author={Bi, Baolong and Huang, Shaohan and Wang, Yiwei and Yang, Tianchi and Zhang, Zihan and Huang, Haizhen and Mei, Lingrui and Fang, Junfeng and Li, Zehao and Wei, Furu and others},
  journal={arXiv preprint arXiv:2412.15280},
  year={2024}
}
          </code></pre>
        </div>
      </div>
    </div>
  </section>
</body>

</html>
